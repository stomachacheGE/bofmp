{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Classifying Names with a Character-Level RNN\n",
    "*********************************************\n",
    "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
    "\n",
    "We will be building and training a basic character-level RNN to classify\n",
    "words. A character-level RNN reads words as a series of characters -\n",
    "outputting a prediction and \"hidden state\" at each step, feeding its\n",
    "previous hidden state into each next step. We take the final prediction\n",
    "to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages\n",
    "of origin, and predict which language a name is from based on the\n",
    "spelling:\n",
    "\n",
    "::\n",
    "\n",
    "    $ python predict.py Hinton\n",
    "    (-0.47) Scottish\n",
    "    (-1.52) English\n",
    "    (-3.57) Irish\n",
    "\n",
    "    $ python predict.py Schmidhuber\n",
    "    (-0.19) German\n",
    "    (-2.48) Czech\n",
    "    (-2.68) Dutch\n",
    "\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and\n",
    "understand Tensors:\n",
    "\n",
    "-  http://pytorch.org/ For installation instructions\n",
    "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
    "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
    "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "-  `The Unreasonable Effectiveness of Recurrent Neural\n",
    "   Networks <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n",
    "   shows a bunch of real life examples\n",
    "-  `Understanding LSTM\n",
    "   Networks <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n",
    "   is about LSTMs specifically but also informative about RNNs in\n",
    "   general\n",
    "\n",
    "Preparing the Data\n",
    "==================\n",
    "\n",
    ".. Note::\n",
    "   Download the data from\n",
    "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
    "   and extract it to the current directory.\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'data/names/Arabic.txt', u'data/names/Portuguese.txt', u'data/names/Scottish.txt', u'data/names/French.txt', u'data/names/Vietnamese.txt', u'data/names/Chinese.txt', u'data/names/Spanish.txt', u'data/names/Greek.txt', u'data/names/Russian.txt', u'data/names/Czech.txt', u'data/names/Polish.txt', u'data/names/English.txt', u'data/names/German.txt', u'data/names/Irish.txt', u'data/names/Korean.txt', u'data/names/Dutch.txt', u'data/names/Italian.txt', u'data/names/Japanese.txt']\n",
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ``category_lines``, a dictionary mapping each category\n",
    "(language) to a list of lines (names). We also kept track of\n",
    "``all_categories`` (just a list of languages) and ``n_categories`` for\n",
    "later reference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Abandonato', u'Abatangelo', u'Abatantuono', u'Abate', u'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning Names into Tensors\n",
    "--------------------------\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size\n",
    "``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     1     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 56 \n",
      "    0     0     0     0     0\n",
      "[torch.cuda.FloatTensor of size 1x57 (GPU 0)]\n",
      "\n",
      "torch.Size([5, 1, 57])\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     1     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 56 \n",
      "    0     0     0     0     0\n",
      "[torch.cuda.FloatTensor of size 1x57 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters).cuda()\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters).cuda()\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())\n",
    "print(lineToTensor('Jones')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Network\n",
    "====================\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "This RNN module (mostly copied from `the PyTorch for Torch users\n",
    "tutorial <https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb>`__)\n",
    "is just 2 linear layers which operate on an input and hidden state, with\n",
    "a LogSoftmax layer after the output.\n",
    "\n",
    ".. figure:: https://i.imgur.com/Z2xbySO.png\n",
    "   :alt: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN (\n",
       "  (i2act): Linear (185 -> 128)\n",
       "  (act2h): Tanh ()\n",
       "  (i2o): Linear (128 -> 18)\n",
       "  (softmax): LogSoftmax ()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2act = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.act2h = nn.Tanh()\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.act2h(self.i2act(combined))\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size).cuda())\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "rnn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "Remember that PyTorch modules operate on Variables rather than straight\n",
    "up Tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    1     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 56 \n",
      "    0     0     0     0     0\n",
      "[torch.cuda.FloatTensor of size 1x57 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 64 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 65 to 77 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 78 to 90 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 91 to 103 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 104 to 116 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 117 to 127 \n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.cuda.FloatTensor of size 1x128 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(letterToTensor('A'))\n",
    "hidden = Variable(torch.zeros(1, n_hidden).cuda())\n",
    "print(input)\n",
    "print(hidden)\n",
    "output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for\n",
    "every step, so we will use ``lineToTensor`` instead of\n",
    "``letterToTensor`` and use slices. This could be further optimized by\n",
    "pre-computing batches of Tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    1     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 56 \n",
      "    0     0     0     0     0\n",
      "[torch.cuda.FloatTensor of size 1x57 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-2.8590 -2.8597 -2.9009 -2.8441 -2.9676 -2.8603 -2.8384 -2.8402 -2.9717 -2.7622\n",
      "\n",
      "Columns 10 to 17 \n",
      "-2.8365 -2.9139 -2.8962 -2.8968 -2.9987 -2.9316 -2.8868 -2.9954\n",
      "[torch.cuda.FloatTensor of size 1x18 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(lineToTensor('Albert'))\n",
    "hidden = Variable(torch.zeros(1, n_hidden).cuda())\n",
    "print(input[0])\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "Preparing for Training\n",
    "----------------------\n",
    "\n",
    "Before going into training we should make a few helper functions. The\n",
    "first is to interpret the output of the network, which we know to be a\n",
    "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
    "of the greatest value:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Czech', 9)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its\n",
    "language):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Russian / line = Yagutkin\n",
      "category = Dutch / line = Marqueringh\n",
      "category = Dutch / line = Kools\n",
      "category = Portuguese / line = Coelho\n",
      "category = Chinese / line = Gao\n",
      "category = Scottish / line = Sutherland\n",
      "category = Irish / line = O'Hanlon\n",
      "category = German / line = Senft\n",
      "category = Spanish / line = Valencia\n",
      "category = Italian / line = Greco\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(LongTensor([all_categories.index(category)]))\n",
    "    line_tensor = Variable(lineToTensor(line))\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Network\n",
    "--------------------\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples,\n",
    "have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and\n",
    "\n",
    "   -  Keep hidden state for next letter\n",
    "\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the\n",
    "``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss for plotting. Since there are 1000s\n",
    "of examples we print only every ``print_every`` examples, and take an\n",
    "average of the loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0% (0m 2s) 2.7601 Merta / Czech ✓\n",
      "1000 1% (0m 5s) 2.8328 Chin / German ✗ (Korean)\n",
      "1500 1% (0m 8s) 3.0431 Sleiman / Dutch ✗ (Arabic)\n",
      "2000 2% (0m 10s) 2.7856 an / Vietnamese ✓\n",
      "2500 2% (0m 13s) 3.0072 Ross / Greek ✗ (Scottish)\n",
      "3000 3% (0m 16s) 2.4876 Lemmi / Italian ✓\n",
      "3500 3% (0m 19s) 2.7755 Abano / Italian ✗ (Spanish)\n",
      "4000 4% (0m 21s) 2.8555 Diakogeorgiou / Korean ✗ (Greek)\n",
      "4500 4% (0m 24s) 2.8285 Woo / Greek ✗ (Korean)\n",
      "5000 5% (0m 27s) 2.8494 Ivory / French ✗ (English)\n",
      "5500 5% (0m 29s) 1.8693 Abatescianni / Italian ✓\n",
      "6000 6% (0m 32s) 1.8660 Okanaya / Japanese ✓\n",
      "6500 6% (0m 35s) 2.5639 Mullen / Dutch ✗ (Irish)\n",
      "7000 7% (0m 38s) 3.2637 O'Shea / Czech ✗ (Irish)\n",
      "7500 7% (0m 40s) 2.5245 Conchobhar / Greek ✗ (Irish)\n",
      "8000 8% (0m 43s) 2.2229 Diarmaid / Russian ✗ (Irish)\n",
      "8500 8% (0m 46s) 2.7552 Fuhrmann / Irish ✗ (German)\n",
      "9000 9% (0m 49s) 2.1632 Rumisek / Polish ✗ (Czech)\n",
      "9500 9% (0m 51s) 0.6768 Ablyakimov / Russian ✓\n",
      "10000 10% (0m 54s) 0.4271 Imagawa / Japanese ✓\n",
      "10500 10% (0m 57s) 1.9837 Vitali / Japanese ✗ (Italian)\n",
      "11000 11% (1m 0s) 0.6822 Hudyakov / Russian ✓\n",
      "11500 11% (1m 2s) 1.2048 Cha / Korean ✓\n",
      "12000 12% (1m 5s) 2.0666 Mach / Vietnamese ✓\n",
      "12500 12% (1m 8s) 1.6184 Neeson / English ✓\n",
      "13000 13% (1m 11s) 2.0350 Tommii / Arabic ✗ (Japanese)\n",
      "13500 13% (1m 14s) 1.9821 Aller / German ✗ (Dutch)\n",
      "14000 14% (1m 16s) 1.9550 Kalb / Arabic ✓\n",
      "14500 14% (1m 19s) 0.6199 Carracci / Italian ✓\n",
      "15000 15% (1m 22s) 1.6132 Abreu / Spanish ✗ (Portuguese)\n",
      "15500 15% (1m 24s) 4.3856 Juhnin / Korean ✗ (Russian)\n",
      "16000 16% (1m 27s) 4.0692 Leon / Korean ✗ (French)\n",
      "16500 16% (1m 30s) 3.6006 Natale / Japanese ✗ (Italian)\n",
      "17000 17% (1m 33s) 1.7970 Osladil / Russian ✗ (Czech)\n",
      "17500 17% (1m 35s) 0.8638 Onishi / Japanese ✓\n",
      "18000 18% (1m 38s) 0.6711 Shim / Korean ✓\n",
      "18500 18% (1m 41s) 0.5176 Kolovos / Greek ✓\n",
      "19000 19% (1m 43s) 0.9466 Sargent / French ✓\n",
      "19500 19% (1m 46s) 1.8287 Hua / Vietnamese ✗ (Chinese)\n",
      "20000 20% (1m 49s) 2.2771 Scott / German ✗ (Scottish)\n",
      "20500 20% (1m 52s) 2.2728 Debenham / Scottish ✗ (English)\n",
      "21000 21% (1m 54s) 0.4140 Arrigucci / Italian ✓\n",
      "21500 21% (1m 57s) 0.4466 Seok / Korean ✓\n",
      "22000 22% (2m 0s) 0.1675 Metrofanis / Greek ✓\n",
      "22500 22% (2m 2s) 0.6763 Kosmatka / Polish ✓\n",
      "23000 23% (2m 5s) 0.7108 an / Vietnamese ✓\n",
      "23500 23% (2m 8s) 0.0986 Roncalli / Italian ✓\n",
      "24000 24% (2m 10s) 0.8464 Than / Vietnamese ✓\n",
      "24500 24% (2m 13s) 0.1570 Absattarov / Russian ✓\n",
      "25000 25% (2m 15s) 0.8375 Vu / Vietnamese ✓\n",
      "25500 25% (2m 18s) 1.2123 Ireson / Scottish ✗ (English)\n",
      "26000 26% (2m 21s) 1.0900 D'cruz / Portuguese ✗ (Spanish)\n",
      "26500 26% (2m 24s) 1.1558 Barros / Greek ✗ (Portuguese)\n",
      "27000 27% (2m 27s) 1.5708 Bellerose / French ✓\n",
      "27500 27% (2m 29s) 4.7292 Comino / Italian ✗ (Greek)\n",
      "28000 28% (2m 32s) 0.1151 Okita / Japanese ✓\n",
      "28500 28% (2m 34s) 2.4473 Mcgregor / French ✗ (Scottish)\n",
      "29000 28% (2m 37s) 1.5774 Langford / Irish ✗ (English)\n",
      "29500 29% (2m 39s) 2.4311 Ryskamp / German ✗ (Dutch)\n",
      "30000 30% (2m 42s) 1.5154 Prchal / Czech ✓\n",
      "30500 30% (2m 45s) 0.9566 Gagnier / French ✓\n",
      "31000 31% (2m 47s) 1.6558 Sierra / Portuguese ✗ (Spanish)\n",
      "31500 31% (2m 49s) 1.2714 Kuiper / German ✗ (Dutch)\n",
      "32000 32% (2m 52s) 0.7909 Niadh / Irish ✓\n",
      "32500 32% (2m 55s) 1.9487 Bell / German ✗ (Scottish)\n",
      "33000 33% (2m 57s) 7.5180 Benetton / Scottish ✗ (Italian)\n",
      "33500 33% (3m 0s) 0.8706 Rong / Korean ✗ (Chinese)\n",
      "34000 34% (3m 2s) 1.3108 Fournier / French ✓\n",
      "34500 34% (3m 5s) 0.0293 Panayiotopoulos / Greek ✓\n",
      "35000 35% (3m 8s) 1.1399 Kong / Korean ✗ (Chinese)\n",
      "35500 35% (3m 11s) 2.8237 Smit / Russian ✗ (Dutch)\n",
      "36000 36% (3m 14s) 1.5181 Tahan / Vietnamese ✗ (Arabic)\n",
      "36500 36% (3m 17s) 2.6153 Noel / Dutch ✗ (French)\n",
      "37000 37% (3m 20s) 1.6833 Shamoun / Irish ✗ (Arabic)\n",
      "37500 37% (3m 23s) 3.7113 Molloy / Scottish ✗ (Irish)\n",
      "38000 38% (3m 26s) 3.2086 Rios / Greek ✗ (Spanish)\n",
      "38500 38% (3m 28s) 1.2650 Rocca / Italian ✓\n",
      "39000 39% (3m 31s) 0.1534 Romijnders / Dutch ✓\n",
      "39500 39% (3m 34s) 0.2726 Awad / Arabic ✓\n",
      "40000 40% (3m 36s) 2.1309 Foss / English ✗ (French)\n",
      "40500 40% (3m 39s) 2.1126 Allard / Scottish ✗ (French)\n",
      "41000 41% (3m 42s) 4.0285 Aitken / Russian ✗ (Scottish)\n",
      "41500 41% (3m 45s) 3.6473 Burns / Dutch ✗ (Scottish)\n",
      "42000 42% (3m 47s) 0.7581 Daviau / French ✓\n",
      "42500 42% (3m 50s) 1.0078 Cathan / Irish ✓\n",
      "43000 43% (3m 53s) 0.5433 Szewc / Polish ✓\n",
      "43500 43% (3m 56s) 0.7107 Boucher / French ✓\n",
      "44000 44% (3m 58s) 1.2861 Flores / Portuguese ✗ (Spanish)\n",
      "44500 44% (4m 1s) 1.0850 Kang / Chinese ✗ (Korean)\n",
      "45000 45% (4m 3s) 0.6441 Kruger / German ✓\n",
      "45500 45% (4m 6s) 1.7718 Brown / German ✗ (Scottish)\n",
      "46000 46% (4m 9s) 0.8574 Zhi / Chinese ✓\n",
      "46500 46% (4m 12s) 1.1211 Bello / Spanish ✓\n",
      "47000 47% (4m 15s) 2.5769 Kerner / German ✗ (Czech)\n",
      "47500 47% (4m 17s) 6.1719 Masin / Arabic ✗ (Italian)\n",
      "48000 48% (4m 20s) 0.0982 Prosdocimi / Italian ✓\n",
      "48500 48% (4m 22s) 1.1707 Vrazel / Czech ✓\n",
      "49000 49% (4m 25s) 1.1065 Bokhoven / Dutch ✓\n",
      "49500 49% (4m 27s) 0.6325 Gomes / Portuguese ✓\n",
      "50000 50% (4m 30s) 0.9600 Rose / French ✓\n",
      "50500 50% (4m 33s) 1.2683 Cameron / Scottish ✓\n",
      "51000 51% (4m 35s) 0.1813 Adamczak / Polish ✓\n",
      "51500 51% (4m 38s) 0.1780 Seighin / Irish ✓\n",
      "52000 52% (4m 41s) 1.9957 Bang / Chinese ✗ (Korean)\n",
      "52500 52% (4m 43s) 1.1519 Sloan / Irish ✓\n",
      "53000 53% (4m 46s) 0.8490 Tong / Vietnamese ✓\n",
      "53500 53% (4m 49s) 0.5828 Dubhain / Irish ✓\n",
      "54000 54% (4m 51s) 0.3806 Palmeiro / Portuguese ✓\n",
      "54500 54% (4m 54s) 0.8204 Montagne / French ✓\n",
      "55000 55% (4m 56s) 0.6099 Slepicka / Czech ✓\n",
      "55500 55% (4m 59s) 0.0795 Armani / Italian ✓\n",
      "56000 56% (5m 2s) 0.3082 Moralez / Spanish ✓\n",
      "56500 56% (5m 4s) 2.5099 Houte / Japanese ✗ (Dutch)\n",
      "57000 56% (5m 7s) 0.5279 Schrader / German ✓\n",
      "57500 57% (5m 10s) 0.2073 Sneijers / Dutch ✓\n",
      "58000 57% (5m 13s) 2.6730 Bureau / Vietnamese ✗ (French)\n",
      "58500 58% (5m 15s) 1.6794 Furness / French ✗ (English)\n",
      "59000 59% (5m 18s) 0.9767 Spijker / Dutch ✓\n",
      "59500 59% (5m 21s) 1.3350 Araullo / Portuguese ✗ (Spanish)\n",
      "60000 60% (5m 23s) 0.1825 Kyritsis / Greek ✓\n",
      "60500 60% (5m 26s) 0.0162 Jaskulski / Polish ✓\n",
      "61000 61% (5m 29s) 0.1924 Paloumbas / Greek ✓\n",
      "61500 61% (5m 31s) 3.2465 Klimes / Dutch ✗ (Czech)\n",
      "62000 62% (5m 34s) 4.6275 Fabian / Irish ✗ (Polish)\n",
      "62500 62% (5m 37s) 0.1252 Slaski / Polish ✓\n",
      "63000 63% (5m 40s) 0.1343 Fukayama / Japanese ✓\n",
      "63500 63% (5m 43s) 9.2002 Tourna / Italian ✗ (Greek)\n",
      "64000 64% (5m 46s) 2.7081 Coma / Spanish ✗ (Czech)\n",
      "64500 64% (5m 48s) 2.1438 De felice / French ✗ (Italian)\n",
      "65000 65% (5m 51s) 0.1219 Ruadhain / Irish ✓\n",
      "65500 65% (5m 54s) 1.4060 Maurice / French ✗ (Irish)\n",
      "66000 66% (5m 57s) 0.3267 Aswad / Arabic ✓\n",
      "66500 66% (5m 59s) 0.0676 La / Vietnamese ✓\n",
      "67000 67% (6m 2s) 1.1715 Higgs / English ✓\n",
      "67500 67% (6m 4s) 2.2932 Evison / Russian ✗ (English)\n",
      "68000 68% (6m 7s) 0.2479 Mai / Chinese ✓\n",
      "68500 68% (6m 9s) 0.6821 Gomes / Portuguese ✓\n",
      "69000 69% (6m 12s) 0.0012 Mulyukov / Russian ✓\n",
      "69500 69% (6m 15s) 2.8903 Allard / Irish ✗ (French)\n",
      "70000 70% (6m 17s) 0.2186 Ryom / Korean ✓\n",
      "70500 70% (6m 20s) 2.2525 Meinhardt / Dutch ✗ (German)\n",
      "71000 71% (6m 22s) 0.0080 Bairamkulov / Russian ✓\n",
      "71500 71% (6m 25s) 0.4577 Madeira / Portuguese ✓\n",
      "72000 72% (6m 28s) 0.6031 Nifterik / Dutch ✓\n",
      "72500 72% (6m 30s) 0.1417 Guan / Chinese ✓\n",
      "73000 73% (6m 33s) 0.0054 Quach / Vietnamese ✓\n",
      "73500 73% (6m 36s) 1.9997 Boutros / Portuguese ✗ (Arabic)\n",
      "74000 74% (6m 38s) 0.1282 Shammas / Arabic ✓\n",
      "74500 74% (6m 41s) 1.1418 Shackleton / English ✓\n",
      "75000 75% (6m 44s) 1.4360 Niftrik / Czech ✗ (Dutch)\n",
      "75500 75% (6m 46s) 0.5925 Moon / Korean ✓\n",
      "76000 76% (6m 49s) 0.0076 Koustoubos / Greek ✓\n",
      "76500 76% (6m 51s) 0.5145 Rompaeij / Dutch ✓\n",
      "77000 77% (6m 54s) 0.1531 Cleirich / Irish ✓\n",
      "77500 77% (6m 56s) 0.2074 Hyata / Japanese ✓\n",
      "78000 78% (6m 59s) 0.4138 Sutherland / Scottish ✓\n",
      "78500 78% (7m 2s) 0.6287 Isa / Arabic ✓\n",
      "79000 79% (7m 5s) 3.1090 Rendon / Irish ✗ (Spanish)\n",
      "79500 79% (7m 7s) 0.2324 Qiu / Chinese ✓\n",
      "80000 80% (7m 10s) 0.9159 Shinko / Japanese ✓\n",
      "80500 80% (7m 12s) 3.2563 Robertson / Scottish ✗ (English)\n",
      "81000 81% (7m 15s) 0.7114 Morrison / Scottish ✓\n",
      "81500 81% (7m 17s) 0.8922 Hladky / Russian ✗ (Czech)\n",
      "82000 82% (7m 20s) 0.1786 Dam / Vietnamese ✓\n",
      "82500 82% (7m 22s) 3.1535 Jares / Spanish ✗ (Czech)\n",
      "83000 83% (7m 25s) 1.5231 Schwarz / German ✗ (Czech)\n",
      "83500 83% (7m 28s) 1.4594 Rigg / English ✓\n",
      "84000 84% (7m 30s) 0.8240 Fraser / Scottish ✓\n",
      "84500 84% (7m 33s) 1.6738 Kang / Chinese ✗ (Korean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000 85% (7m 36s) 0.0505 O'Connor / Irish ✓\n",
      "85500 85% (7m 39s) 1.1614 Cruz / Portuguese ✗ (Spanish)\n",
      "86000 86% (7m 41s) 0.2268 Moreno / Spanish ✓\n",
      "86500 86% (7m 44s) 1.5902 Daugherty / Irish ✗ (English)\n",
      "87000 87% (7m 47s) 0.1087 Xie / Chinese ✓\n",
      "87500 87% (7m 49s) 1.6436 Porras / Portuguese ✗ (Spanish)\n",
      "88000 88% (7m 52s) 0.3136 Thien / Chinese ✓\n",
      "88500 88% (7m 55s) 0.4984 Cardozo / Portuguese ✓\n",
      "89000 89% (7m 58s) 0.3907 Fernandes / Portuguese ✓\n",
      "89500 89% (8m 0s) 1.2825 Franco / Spanish ✗ (Portuguese)\n",
      "90000 90% (8m 3s) 3.6068 De fiore / French ✗ (Italian)\n",
      "90500 90% (8m 6s) 1.9996 Chmiel / Irish ✗ (Polish)\n",
      "91000 91% (8m 8s) 0.6876 Miazga / Polish ✓\n",
      "91500 91% (8m 11s) 0.5692 Escamilla / Spanish ✓\n",
      "92000 92% (8m 14s) 0.1790 Allan / Scottish ✓\n",
      "92500 92% (8m 16s) 0.0080 Malouf / Arabic ✓\n",
      "93000 93% (8m 19s) 0.5964 Mancuso / Italian ✓\n",
      "93500 93% (8m 22s) 0.1521 Zuraw / Polish ✓\n",
      "94000 94% (8m 24s) 0.5088 Arreola / Spanish ✓\n",
      "94500 94% (8m 27s) 0.0055 Khouri / Arabic ✓\n",
      "95000 95% (8m 30s) 0.1973 Nelissen / Dutch ✓\n",
      "95500 95% (8m 32s) 2.4643 Sung / Chinese ✗ (Korean)\n",
      "96000 96% (8m 35s) 0.4318 Fernandez / Spanish ✓\n",
      "96500 96% (8m 38s) 3.4047 Uss / German ✗ (Russian)\n",
      "97000 97% (8m 40s) 0.1922 Santos / Portuguese ✓\n",
      "97500 97% (8m 43s) 0.1782 Ventura / Portuguese ✓\n",
      "98000 98% (8m 46s) 3.6996 Soler / Dutch ✗ (Spanish)\n",
      "98500 98% (8m 49s) 0.9092 Lesauvage / French ✓\n",
      "99000 99% (8m 51s) 2.2012 Mullen / Irish ✗ (English)\n",
      "99500 99% (8m 53s) 0.3263 Santana / Portuguese ✓\n",
      "100000 100% (8m 56s) 0.1309 Tow / Chinese ✓\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 500\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Results\n",
    "--------------------\n",
    "\n",
    "Plotting the historical loss from ``all_losses`` shows the network\n",
    "learning:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4dc8527bd0>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK6WUG/r//2Y6fpuxcGgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4dc867c0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Results\n",
    "======================\n",
    "\n",
    "To see how well the network performs on different categories, we will\n",
    "create a confusion matrix, indicating for every actual language (rows)\n",
    "which language the network guesses (columns). To calculate the confusion\n",
    "matrix a bunch of samples are run through the network with\n",
    "``evaluate()``, which is the same as ``train()`` minus the backprop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Variable data has to be a tensor, but got instancemethod",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-978412f08ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_confusion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomTrainingExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategoryFromOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcategory_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_categories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-978412f08ecc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(line_tensor)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Just return an output given a line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-7bcb01d303c5>\u001b[0m in \u001b[0;36minitHidden\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mn_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got instancemethod"
     ]
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish\n",
    "for Italian. It seems to do very well with Greek, and very poorly with\n",
    "English (perhaps because of overlap with other languages).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on User Input\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(lineToTensor(input_line)))\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.data.topk(n_predictions, 1, True)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][i]\n",
    "        category_index = topi[0][i]\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final versions of the scripts `in the Practical PyTorch\n",
    "repo <https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification>`__\n",
    "split the above code into a few files:\n",
    "\n",
    "-  ``data.py`` (loads files)\n",
    "-  ``model.py`` (defines the RNN)\n",
    "-  ``train.py`` (runs training)\n",
    "-  ``predict.py`` (runs ``predict()`` with command line arguments)\n",
    "-  ``server.py`` (serve prediction as a JSON API with bottle.py)\n",
    "\n",
    "Run ``train.py`` to train and save the network.\n",
    "\n",
    "Run ``predict.py`` with a name to view predictions:\n",
    "\n",
    "::\n",
    "\n",
    "    $ python predict.py Hazaki\n",
    "    (-0.42) Japanese\n",
    "    (-1.39) Polish\n",
    "    (-3.51) Czech\n",
    "\n",
    "Run ``server.py`` and visit http://localhost:5533/Yourname to get JSON\n",
    "output of predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset of line -> category, for example:\n",
    "\n",
    "   -  Any word -> language\n",
    "   -  First name -> gender\n",
    "   -  Character name -> writer\n",
    "   -  Page title -> blog or subreddit\n",
    "\n",
    "-  Get better results with a bigger and/or better shaped network\n",
    "\n",
    "   -  Add more linear layers\n",
    "   -  Try the ``nn.LSTM`` and ``nn.GRU`` layers\n",
    "   -  Combine multiple of these RNNs as a higher level network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
